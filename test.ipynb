{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import PriorityExperienceReplay\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.gen_math_ops import Exp\n",
    "\n",
    "from actor import Actor\n",
    "from critic import Critic\n",
    "from replay_memory import ReplayMemory\n",
    "#from embedding import MovieGenreEmbedding, UserMovieEmbedding\n",
    "from state_representation import DRRAveStateRepresentation\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class DRRAgent:\n",
    "    \n",
    "    def __init__(self, env, users_num, items_num, state_size,ipc_to_description, is_test=False, use_wandb=False):\n",
    "        \n",
    "        self.env = env\n",
    "        self.ipc_to_description = ipc_to_description\n",
    "        self.users_num = users_num\n",
    "        self.items_num = items_num\n",
    "        \n",
    "        ##self.embedding_dim = 100\n",
    "        self.embedding_dim = 768\n",
    "        self.actor_hidden_dim = 128\n",
    "        self.actor_learning_rate = 0.001\n",
    "        self.critic_hidden_dim = 128\n",
    "        self.critic_learning_rate = 0.001\n",
    "        self.discount_factor = 0.9\n",
    "        self.tau = 0.001\n",
    "\n",
    "        self.replay_memory_size = 1000000\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.actor = Actor(self.embedding_dim, self.actor_hidden_dim, self.actor_learning_rate, state_size, self.tau)\n",
    "        self.critic = Critic(self.critic_hidden_dim, self.critic_learning_rate, self.embedding_dim, self.tau)\n",
    "        \n",
    "        # self.m_embedding_network = MovieGenreEmbedding(items_num, 19, self.embedding_dim)\n",
    "        # self.m_embedding_network([np.zeros((1,)),np.zeros((1,))])\n",
    "        # self.m_embedding_network.load_weights('/home/diominor/Workspace/DRR/save_weights/m_g_model_weights.h5')\n",
    "\n",
    "        ##self.embedding_network = UserMovieEmbedding(users_num, items_num, self.embedding_dim)\n",
    "        ##self.embedding_network([np.zeros((1,)),np.zeros((1,))])\n",
    "        # self.embedding_network = UserMovieEmbedding(users_num, self.embedding_dim)\n",
    "        # self.embedding_network([np.zeros((1)),np.zeros((1,100))])\n",
    "        self.save_model_weight_dir = f\"./save_model/trail-{datetime.now().strftime('%Y-%m-%d-%H')}\"\n",
    "        if not os.path.exists(self.save_model_weight_dir):\n",
    "            os.makedirs(os.path.join(self.save_model_weight_dir, 'imagess'))\n",
    "        ###embedding_save_file_dir = './save_weights/user_movie_embedding_case4.h5'\n",
    "        ###assert os.path.exists(embedding_save_file_dir), f\"embedding save file directory: '{embedding_save_file_dir}' is wrong.\"\n",
    "        ###self.embedding_network.load_weights(embedding_save_file_dir)\n",
    "\n",
    "        self.srm_ave = DRRAveStateRepresentation(self.embedding_dim)\n",
    "        self.srm_ave([np.zeros((1, 768,)),np.zeros((1,state_size, 768))])\n",
    "\n",
    "        # PER\n",
    "        self.buffer = PriorityExperienceReplay(self.replay_memory_size, self.embedding_dim)\n",
    "        self.epsilon_for_priority = 1e-6\n",
    "\n",
    "        # ε-탐욕 탐색 하이퍼파라미터 ε-greedy exploration hyperparameter\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_decay = (self.epsilon - 0.1)/500000\n",
    "        self.std = 1.5\n",
    "\n",
    "        self.is_test = is_test\n",
    "\n",
    "        # wandb\n",
    "        self.use_wandb = use_wandb\n",
    "        if use_wandb:\n",
    "            wandb.init(project=\"drr\", \n",
    "            entity=\"diominor\",\n",
    "            config={'users_num':users_num,\n",
    "            'items_num' : items_num,\n",
    "            'state_size' : state_size,\n",
    "            'embedding_dim' : self.embedding_dim,\n",
    "            'actor_hidden_dim' : self.actor_hidden_dim,\n",
    "            'actor_learning_rate' : self.actor_learning_rate,\n",
    "            'critic_hidden_dim' : self.critic_hidden_dim,\n",
    "            'critic_learning_rate' : self.critic_learning_rate,\n",
    "            'discount_factor' : self.discount_factor,\n",
    "            'tau' : self.tau,\n",
    "            'replay_memory_size' : self.replay_memory_size,\n",
    "            'batch_size' : self.batch_size,\n",
    "            'std_for_exploration': self.std})\n",
    "\n",
    "    def calculate_td_target(self, rewards, q_values, dones):\n",
    "        y_t = np.copy(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            y_t[i] = rewards[i] + (1 - dones[i])*(self.discount_factor * q_values[i])\n",
    "        return y_t\n",
    "\n",
    "    def recommend_item(self, action, recommended_items, top_k=False, items_ids=None):\n",
    "        \n",
    "        \n",
    "        if items_ids == None:\n",
    "          all_user_id, all_items_ids, all_done = self.env.reset_all()\n",
    "          items_ids=[item for item in all_items_ids if item not in recommended_items]\n",
    "          #items_ids=all_items_ids\n",
    "          #print('recommend_item : ',items_ids)\n",
    "          #items_ids = np.array(list(set(i for i in range(self.items_num)) - recommended_items))\n",
    "\n",
    "        ###items_ebs = self.embedding_network.get_layer('movie_embedding')(items_ids)\n",
    "        items_ebs = self.bert(items_ids)\n",
    "        # items_ebs = self.m_embedding_network.get_layer('movie_embedding')(items_ids)\n",
    "        action = tf.transpose(action, perm=(1,0))\n",
    "        if top_k:\n",
    "            item_indice = np.argsort(tf.transpose(tf.keras.backend.dot(items_ebs, action), perm=(1,0)))[0][-top_k:]\n",
    "            return items_ids[item_indice]\n",
    "        else:\n",
    "            item_idx = np.argmax(tf.keras.backend.dot(items_ebs, action))\n",
    "            return items_ids[item_idx]\n",
    "    \n",
    "\n",
    "    def bert(self,items_ids):\n",
    "        \n",
    "      model_name='bert-base-multilingual-cased'\n",
    "      tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "      model = BertModel.from_pretrained(model_name)\n",
    "      items_array = []\n",
    "\n",
    "      for id in items_ids:\n",
    "        try:\n",
    "          sentence = self.ipc_to_description[id]\n",
    "        except KeyError:\n",
    "          #print(f\"No description found for IPC {id}\")\n",
    "          continue\n",
    "        \n",
    "        sentence = self.ipc_to_description[id]\n",
    "        #print(sentence)\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embedded_vector =  outputs.last_hidden_state.mean(dim=1)\n",
    "        items_array.append(embedded_vector)\n",
    "      \n",
    "      items_tensor = torch.cat(items_array, dim=0)\n",
    "      \n",
    "      #averaged_embedding = torch.mean(items_tensor.view(4, -1, 768), dim=1)\n",
    "      #averaged_embedding = self.pad_and_average_pooling(items_array)\n",
    "      return items_tensor  \n",
    "    \n",
    "    def train(self, max_episode_num, top_k=False, load_model=False):\n",
    "        # 타겟 네트워크들 초기화\n",
    "        self.actor.update_target_network()\n",
    "        self.critic.update_target_network()\n",
    "\n",
    "        if load_model:\n",
    "            self.load_model(\"/home/diominor/Workspace/DRR/save_weights/actor_50000.h5\", \"/home/diominor/Workspace/DRR/save_weights/critic_50000.h5\")\n",
    "            print('Completely load weights!')\n",
    "\n",
    "        episodic_precision_history = []\n",
    "\n",
    "        for episode in range(max_episode_num):\n",
    "            # episodic reward 리셋\n",
    "            episode_reward = 0\n",
    "            correct_count = 0\n",
    "            steps = 0\n",
    "            q_loss = 0\n",
    "            mean_action = 0\n",
    "            # Environment 리셋\n",
    "            user_id, items_ids, done = self.env.reset()\n",
    "            print(f'user_id : {user_id}, rated_items_length:{len(self.env.user_items)}')\n",
    "            #print('items : ', self.env.get_items_names(items_ids))\n",
    "            #print('sentence: ',self.ipc_to_description[items_ids])\n",
    "            #print('ids : ', items_ids)\n",
    "            while not done:\n",
    "                \n",
    "                # Observe current state & Find action\n",
    "                ## Embedding 해주기\n",
    "                ###user_eb = self.embedding_network.get_layer('user_embedding')(np.array(user_id))\n",
    "                user_eb = np.zeros((1, 768,))\n",
    "                ###items_eb = self.embedding_network.get_layer('movie_embedding')(np.array(items_ids))\n",
    "                #input_bert=items_ids.tolist()\n",
    "                items_eb = self.bert(items_ids)\n",
    "                #print(items_eb.shape)\n",
    "                # items_eb = self.m_embedding_network.get_layer('movie_embedding')(np.array(items_ids))\n",
    "                ## SRM으로 state 출력\n",
    "                state = self.srm_ave([np.expand_dims(user_eb, axis=0), np.expand_dims(items_eb, axis=0)])\n",
    "                print('state shape : ', state.shape)\n",
    "                #print(state)\n",
    "\n",
    "                ## Action(ranking score) 출력\n",
    "                action = self.actor.network(state)\n",
    "                #print('action : ',action)\n",
    "\n",
    "                ## ε-greedy exploration\n",
    "                if self.epsilon > np.random.uniform() and not self.is_test:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                    action += np.random.normal(0,self.std,size=action.shape)\n",
    "\n",
    "                ## Item 추천\n",
    "                #print(\"기존 추천 : \", self.env.recommended_items)\n",
    "                #print(\"items: \", items_ids)\n",
    "                recommended_item = self.recommend_item(action, self.env.recommended_items, top_k=top_k)\n",
    "                \n",
    "                # Calculate reward & observe new state (in env)\n",
    "                ## Step\n",
    "                print('recommended : ',recommended_item)\n",
    "                #print(\"1회 완료\")\n",
    "                \n",
    "                next_items_ids, reward, done, _ = self.env.step(recommended_item,top_k=top_k)\n",
    "                print(done)\n",
    "                if top_k:\n",
    "                    reward = np.sum(reward)\n",
    "\n",
    "                \n",
    "\n",
    "                # get next_state\n",
    "                ###next_items_eb = self.embedding_network.get_layer('movie_embedding')(np.array(next_items_ids))\n",
    "\n",
    "                next_items_ids=next_items_ids[:4]\n",
    "                #print('next_ids : ',next_items_ids)\n",
    "                #print('reward : ', reward)\n",
    "                next_items_eb = self.bert(next_items_ids)\n",
    "                # next_items_eb = self.m_embedding_network.get_layer('movie_embedding')(np.array(next_items_ids))\n",
    "                next_state = self.srm_ave([np.expand_dims(user_eb, axis=0), np.expand_dims(next_items_eb, axis=0)])\n",
    "\n",
    "                # buffer에 저장\n",
    "                self.buffer.append(state, action, reward, next_state, done)\n",
    "                \n",
    "                if self.buffer.crt_idx > 1 or self.buffer.is_full:\n",
    "                    # Sample a minibatch\n",
    "                    batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, weight_batch, index_batch = self.buffer.sample(self.batch_size)\n",
    "\n",
    "                    # Set TD targets\n",
    "                    target_next_action= self.actor.target_network(batch_next_states)\n",
    "                    qs = self.critic.network([target_next_action, batch_next_states])\n",
    "                    target_qs = self.critic.target_network([target_next_action, batch_next_states])\n",
    "                    min_qs = tf.raw_ops.Min(input=tf.concat([target_qs, qs], axis=1), axis=1, keep_dims=True) # Double Q method\n",
    "                    td_targets = self.calculate_td_target(batch_rewards, min_qs, batch_dones)\n",
    "        \n",
    "                    # Update priority\n",
    "                    for (p, i) in zip(td_targets, index_batch):\n",
    "                        self.buffer.update_priority(abs(p[0]) + self.epsilon_for_priority, i)\n",
    "\n",
    "                    # print(weight_batch.shape)\n",
    "                    # print(td_targets.shape)\n",
    "                    # raise Exception\n",
    "                    # Update critic network\n",
    "                    q_loss += self.critic.train([batch_actions, batch_states], td_targets, weight_batch)\n",
    "                    \n",
    "                    # Update actor network\n",
    "                    s_grads = self.critic.dq_da([batch_actions, batch_states])\n",
    "                    self.actor.train(batch_states, s_grads)\n",
    "                    self.actor.update_target_network()\n",
    "                    self.critic.update_target_network()\n",
    "\n",
    "                items_ids = next_items_ids\n",
    "                episode_reward += reward\n",
    "                mean_action += np.sum(action[0])/(len(action[0]))\n",
    "                steps += 1\n",
    "\n",
    "                if reward > 0:\n",
    "                    correct_count += 1\n",
    "                \n",
    "                print(f'recommended items : {len(self.env.recommended_items)},  epsilon : {self.epsilon:0.3f}, reward : {reward:+}', end='\\r')\n",
    "\n",
    "                if done:\n",
    "                    print()\n",
    "                    precision = int(correct_count/steps * 100)\n",
    "                    print(f'{episode}/{max_episode_num}, precision : {precision:2}%, total_reward:{episode_reward}, q_loss : {q_loss/steps}, mean_action : {mean_action/steps}')\n",
    "                    if self.use_wandb:\n",
    "                        wandb.log({'precision':precision, 'total_reward':episode_reward, 'epsilone': self.epsilon, 'q_loss' : q_loss/steps, 'mean_action' : mean_action/steps})\n",
    "                    episodic_precision_history.append(precision)\n",
    "             \n",
    "            if (episode+1)%50 == 0:\n",
    "                plt.plot(episodic_precision_history)\n",
    "                plt.savefig(os.path.join(self.save_model_weight_dir, f'images/training_precision_%_top_5.png'))\n",
    "\n",
    "            if (episode+1)%1000 == 0 or episode == max_episode_num-1:\n",
    "                self.save_model(os.path.join(self.save_model_weight_dir, f'actor_{episode+1}_fixed.h5'),\n",
    "                                os.path.join(self.save_model_weight_dir, f'critic_{episode+1}_fixed.h5'))\n",
    "\n",
    "    def save_model(self, actor_path, critic_path):\n",
    "        self.actor.save_weights(actor_path)\n",
    "        self.critic.save_weights(critic_path)\n",
    "        \n",
    "    def load_model(self, actor_path, critic_path):\n",
    "        self.actor.load_weights(actor_path)\n",
    "        self.critic.load_weights(critic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1993bbc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'replay_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreplay_buffer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PriorityExperienceReplay\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'replay_buffer'"
     ]
    }
   ],
   "source": [
    "from replay_buffer import PriorityExperienceReplay\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.gen_math_ops import Exp\n",
    "\n",
    "from actor import Actor\n",
    "from critic import Critic\n",
    "from replay_memory import ReplayMemory\n",
    "#from embedding import MovieGenreEmbedding, UserMovieEmbedding\n",
    "from state_representation import DRRAveStateRepresentation\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb044ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
